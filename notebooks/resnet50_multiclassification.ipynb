{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12ab3f98-7c07-4407-9eb9-53b84ba938f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import timm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from torch.utils.data import Subset\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import silhouette_score\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51791893-e300-4cad-84f8-1390abcf79cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3   # N x N tiling\n",
    "TRT_SIZE = 224   # 224 x 224 image size for ResNet50 input\n",
    "\n",
    "project_root = os.path.abspath('.')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root) \n",
    "\n",
    "training_data_path = os.path.join(project_root, \"PlantCLEF2025_data/images_max_side_800\")\n",
    "inference_data_path = os.path.join(project_root, \"PlantCLEF2025_data/test_images/images\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((TRT_SIZE,TRT_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=(0.485, 0.456, 0.406),\n",
    "            std=(0.229, 0.224, 0.225))\n",
    "        ])\n",
    "\n",
    "def tile_image_nxn(img, tiles_per_side=3, target_size=224):\n",
    "    w, h = img.size\n",
    "\n",
    "    tile_w = w / tiles_per_side\n",
    "    tile_h = h / tiles_per_side\n",
    "\n",
    "    tiles = []\n",
    "\n",
    "    for row in range(tiles_per_side):\n",
    "        for col in range(tiles_per_side):\n",
    "            left = int(col * tile_w)\n",
    "            top = int(row * tile_h)\n",
    "            right = int((col + 1) * tile_w)\n",
    "            bottom = int((row + 1) * tile_h)\n",
    "\n",
    "            tile = img.crop((left, top, right, bottom))\n",
    "            tile = tile.resize((target_size, target_size), Image.BICUBIC)\n",
    "            tiles.append(tile)\n",
    "\n",
    "    return tiles\n",
    "\n",
    "\n",
    "class QuadratNxNDataset():\n",
    "    def __init__(self, datafolder, transform, tiles_per_side=3, target_size=224):\n",
    "        self.paths = sorted([str(p) for p in Path(datafolder).glob(\"*.*\")])  # list of absolut paths for all images in the folder\n",
    "        self.transform = transform\n",
    "        self.tiles_per_side = tiles_per_side\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "        tiles = tile_image_nxn(\n",
    "            img,\n",
    "            tiles_per_side=self.tiles_per_side,\n",
    "            target_size=self.target_size\n",
    "        )\n",
    "\n",
    "        tiles = [self.transform(t) for t in tiles]\n",
    "        tiles = torch.stack(tiles)  # shape [N*N, 3, 224, 224]\n",
    "\n",
    "        return tiles, path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b2c9b2-cec7-4be4-895c-b628cb806819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the tile_image_nxn function\n",
    "# img = Image.open('2024-CEV3-20240602.jpg')\n",
    "# plt.imshow(img)\n",
    "\n",
    "# tiles = tile_image_nxn(img, tiles_per_side=3, target_size=518)\n",
    "# fig, axes = plt.subplots(3, 3)\n",
    "# axes = axes.flatten()\n",
    "# for i, tile in enumerate(tiles):\n",
    "#     axes[i].imshow(tile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50f00312-bbd5-40e3-b48c-2aa1196d8ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the test/quadrat data\n",
    "inference_loader = DataLoader(\n",
    "        QuadratNxNDataset(\n",
    "            inference_data_path,\n",
    "            transform,\n",
    "            tiles_per_side=N,\n",
    "            target_size=TRT_SIZE\n",
    "        ),\n",
    "        batch_size=1,  # since we are processing 9 tiles for each sample/quadrat image (1 image = 1 batch)\n",
    "        shuffle=False,   # critical for inference to keep tiles in order\n",
    "        num_workers=4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a36a00c9-e612-4920-9575-98b1695dbc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch data shape: torch.Size([1, 9, 3, 224, 224])\n",
      "Batch label - path for quadrat image: ('/sfs/weka/scratch/hl9h/PlantCLEF2025_data/test_images/images/2024-CEV3-20240602.jpg',)\n"
     ]
    }
   ],
   "source": [
    "# Check batch shape\n",
    "data_batch, labels_batch = next(iter(inference_loader))\n",
    "\n",
    "print(f\"Batch data shape: {data_batch.shape}\")   # (Batch_size x num_Tiles x num_Channels x Width x Height)\n",
    "print(f\"Batch label - path for quadrat image: {labels_batch}\")   # (1 image = 1 batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d36473e-938b-49b0-81cc-d194206b41ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the labels and indices form the single-plant training data set\n",
    "print(f\"Loading data from: {training_data_path}\")\n",
    "    try:\n",
    "        # Create the full training dataset\n",
    "        full_training = datasets.ImageFolder(\n",
    "            training_data_path,\n",
    "            transform=transform # Use validation transform for initial loading\n",
    "        )\n",
    "\n",
    "        # Check if dataset is empty\n",
    "        if not full_dataset.samples:\n",
    "            print(f\"ERROR: No images found in {self.data_dir}.\")\n",
    "            return\n",
    "\n",
    "        classes = full_training.classes   # list of all class names (sorted alphabetically)\n",
    "        mapping_dict = full_training.class_to_idx   # the mapping dictionary from class name to index\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred loading data: {e}\")\n",
    "        return\n",
    "\n",
    "# Reverse Mapping (Index to Class Name)\n",
    "def idx_to_class(idx, mapping_dict)\n",
    "    idx_to_class = {v: k for k, v in mapping_dict.items()}\n",
    "    return idx_to_class[idx]\n",
    "\n",
    "# Get top K indices from an array\n",
    "def get_topk(k=3, array):\n",
    "    topk_idx = array.argsort()[-k:][::-1]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7785b49-ca11-4bb7-beb4-19fab64dcfa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "207b555f-ea54-461d-9c74-5d3be6d736ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load the fine tuned ResNet50 parameters\n",
    "from resnet50 import resnet50\n",
    "resnet50_finetuned = resnet50.get_resnet50_pretrained(num_classes=NUM_CLASS, fine_tune=True)\n",
    "state_dict = torch.load('resnet50/resnet50_finetuned_plantCLEF_ep30.pth', weights_only=True)  # weights_only=True is recommended for security\n",
    "resnet50_finetuned.load_state_dict(state_dict)\n",
    "\n",
    "resnet50_finetuned.eval().to(device)\n",
    "for p in resnet50_finetuned.parameters():\n",
    "    p.requires_grad = False\n",
    "for batch, path in tqdm(inference_loader, desc=\"Testing\"):  # each batch is one quadrat image\n",
    "    path = path[0]\n",
    "    batch = batch.to(device)\n",
    "    logits = model(batch)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        imgs = imgs.to(device)\n",
    "        feats = model.forward_features(imgs)\n",
    "        if feats.ndim == 3:\n",
    "            cls_embs = feats[:, 0, :]  # [CLS] token\n",
    "        else:\n",
    "            cls_embs = feats\n",
    "        all_embs.append(cls_embs.cpu().numpy())\n",
    "        all_labels.append(labels.numpy())\n",
    "    return np.concatenate(all_embs), np.concatenate(all_labels)\n",
    "\n",
    "\n",
    "def aggregate_predictions(tile_preds, tile_paths, tiles_per_quadrat=9, min_votes=1):\n",
    "    quadrat_to_species = defaultdict(list)\n",
    "    for i in range(0, len(tile_preds), tiles_per_quadrat):\n",
    "        preds_for_quadrat = tile_preds[i:i+tiles_per_quadrat]\n",
    "        path = tile_paths[i]\n",
    "        quadrat_id = Path(path).stem\n",
    "        # Flatten tile predictions: each tile has k neighbors\n",
    "        flat_preds = preds_for_quadrat.flatten()\n",
    "        # Voting\n",
    "        counter = Counter(flat_preds)\n",
    "        species = [sp for sp, count in counter.items() if count >= min_votes]\n",
    "        quadrat_to_species[quadrat_id] = species\n",
    "    return quadrat_to_species\n",
    "\n",
    "def write_submission(pred_dict, out_csv=\"submission.csv\"):\n",
    "    with open(out_csv, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"quadrat_id\", \"species_ids\"])\n",
    "        for quad, species in pred_dict.items():\n",
    "            s = \"[\" + \", \".join(str(x) for x in species) + \"]\"\n",
    "            writer.writerow([quad, s])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd9404c4-50f4-49dd-b9fd-7428e3916666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing inference dataset for a 3x3 grid.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m IMG_SIZE = \u001b[32m224\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mloading\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m quadrat\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m inference_set = \u001b[43mquadrat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sfs/weka/scratch/hl9h/loading/quadrat.py:158\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    148\u001b[39m inference_transform = transforms.Compose([\n\u001b[32m    149\u001b[39m     transforms.Resize((IMG_SIZE, IMG_SIZE)), \u001b[38;5;66;03m# Crucial\u001b[39;00m\n\u001b[32m    150\u001b[39m     transforms.ToTensor(),\n\u001b[32m    151\u001b[39m     transforms.Normalize(mean=[\u001b[32m0.485\u001b[39m, \u001b[32m0.456\u001b[39m, \u001b[32m0.406\u001b[39m],\n\u001b[32m    152\u001b[39m                          std=[\u001b[32m0.229\u001b[39m, \u001b[32m0.224\u001b[39m, \u001b[32m0.225\u001b[39m])\n\u001b[32m    153\u001b[39m ])\n\u001b[32m    156\u001b[39m \u001b[38;5;66;03m#QUADRAT_DIR = \"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m quadrat_test_set = \u001b[43mQuadratTilingDataset_Inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mQUADRAT_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrid_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43minference_transform\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(quadrat_test_set) == \u001b[32m0\u001b[39m:\n\u001b[32m    165\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDataset is empty. Exiting.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sfs/weka/scratch/hl9h/loading/quadrat.py:51\u001b[39m, in \u001b[36mQuadratTilingDataset_Inference.__init__\u001b[39m\u001b[34m(self, data_dir, grid_size, transform)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28mself\u001b[39m.num_tiles = \u001b[38;5;28mself\u001b[39m.num_rows * \u001b[38;5;28mself\u001b[39m.num_cols\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInitializing inference dataset for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.num_rows\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mx\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.num_cols\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m grid.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sfs/weka/scratch/hl9h/loading/quadrat.py:62\u001b[39m, in \u001b[36mQuadratTilingDataset_Inference._create_samples\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Find all common image types\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ext \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33m*.jpg\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m*.jpeg\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m*.png\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m*.tif\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m*.JPG\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m*.JPEG\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m      img_paths.extend(\u001b[43mglob\u001b[49m.glob(os.path.join(\u001b[38;5;28mself\u001b[39m.data_dir, ext)))\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m img_path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(img_paths): \u001b[38;5;66;03m# Sort to make it reproducible\u001b[39;00m\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_tiles): \u001b[38;5;66;03m# e.g., 0-8\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ce4d59-d71a-4ea5-8fdb-3a9d686bdff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = timm.create_model(\"timm/vit_base_patch14_reg4_dinov2.lvd142m\", pretrained=True)\n",
    "    #model = timm.create_model(\"vit_base_patch16_224\", pretrained=True)\n",
    "    checkpoint_path = '/home/jme3qd/Downloads/model_best.pth.tar'\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False) # Load to CPU first\n",
    "\n",
    "    if 'state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['state_dict']\n",
    "    else:\n",
    "        state_dict = checkpoint # Assume the checkpoint itself is the state_dict\n",
    "\n",
    "    # 3. Load the state dictionary into the model\n",
    "    model.load_state_dict(state_dict,strict=False)\n",
    "\n",
    "    model.eval().to(device)\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7094a77-0a29-4792-84bd-f7ab31cd773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e086a72-ee3c-4bd0-b2db-948c21a8fb96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf8f588-70d3-44e0-9e89-5ffe0a67aaa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43388f7-562b-4fe9-8c83-7cc5cebdbb27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfeca76-4a99-4fd7-80a9-2005d0746dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d986da-1edf-4e43-8673-87fea159462e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e870dca-e6f2-4763-a907-f60861837939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceb367f-7825-4f95-8485-7e8a8eb4ce6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
